{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Lane Detection using Hough Transform in OpenCV\n",
    "\n",
    "---\n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, Hochschule für Technik Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "The following notebook is based on Udacity's *Self-Driving Car Nanodegree*.\n",
    "\n",
    "In this exercise, we implement and experiment with several methods to find lanes lines in street images. Let's take a look at our first example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing all the relevant imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Read in the image and print some stats\n",
    "image = mpimg.imread('images/street.jpg')\n",
    "print('This image is: ', type(image), \n",
    "         'with dimensions:', image.shape)\n",
    "\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) #grayscale conversion\n",
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Color Selection\n",
    "\n",
    "We want to detect the white lane lines. So the simplest idea is to use a color threshold, keeping only the white pixels. So I define a color threshold in the variables red_threshold, green_threshold, and blue_threshold and populate rgb_threshold with these values. This vector contains the minimum values for red, green, and blue (R,G,B) that I will allow in my selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the x and y size and make a copy of the image\n",
    "ysize = image.shape[0]\n",
    "xsize = image.shape[1]\n",
    "color_select = np.copy(image)\n",
    "\n",
    "# Define color selection criteria\n",
    "red_threshold = 0\n",
    "green_threshold = 0\n",
    "blue_threshold = 0\n",
    "\n",
    "rgb_threshold = [red_threshold, green_threshold, blue_threshold]\n",
    "\n",
    "# Do a boolean or with the \"|\" character to identify\n",
    "# pixels below the thresholds\n",
    "thresholds = (image[:,:,0] < rgb_threshold[0]) \\\n",
    "            | (image[:,:,1] < rgb_threshold[1]) \\\n",
    "            | (image[:,:,2] < rgb_threshold[2])\n",
    "color_select[thresholds] = [0,0,0]\n",
    "\n",
    "# Display the image                 \n",
    "plt.imshow(color_select)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result, color_select, is an image in which pixels that were above the threshold have been retained, and pixels below the threshold have been blacked out.\n",
    "In the code snippet above, red_threshold, green_threshold and blue_threshold are all set to 0, which implies all pixels will be included in the selection. \n",
    "\n",
    "**TODO:** Modify the values of red_threshold, green_threshold and blue_threshold above until you retain as much of the lane lines as possible while dropping everything else.\n",
    "\n",
    "**HINT:** You can split the image into separate color channels and visualize them to get clues about which color channel is most important here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coding up a Region of Interest Mask\n",
    "\n",
    "Awesome! Now you've seen that with a simple color selection we have managed to eliminate almost everything in the image except the lane lines. \n",
    "At this point, however, it would still be tricky to extract the exact lines automatically, because we still have some other objects detected around the periphery that aren't lane lines.\n",
    "\n",
    "In this case, I'll assume that the front facing camera that took the image is mounted in a fixed position on the car, such that the lane lines will always appear in the same general region of the image. Next, I'll take advantage of this by adding a criterion to only consider pixels for color selection in the region where we expect to find the lane lines.\n",
    "\n",
    "Check out the code below. The variables left_bottom, right_bottom, and apex represent the vertices of a triangular region that I would like to retain for my color selection, while masking everything else out. Here I'm using a triangular mask to illustrate the simplest case, but later you'll use a quadrilateral, and in principle, you could use any polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the image\n",
    "region_select = np.copy(image)\n",
    "\n",
    "# Define a triangle region of interest \n",
    "# Keep in mind the origin (x=0, y=0) is in the upper left in image processing\n",
    "# Note: if you run this code, you'll find these are not sensible values!!\n",
    "# But you'll get a chance to play with them soon in a quiz \n",
    "left_bottom = [0, 539]\n",
    "right_bottom = [900, 300]\n",
    "apex = [400, 0]\n",
    "\n",
    "# Fit lines (y=Ax+B) to identify the  3 sided region of interest\n",
    "# np.polyfit() returns the coefficients [A, B] of the fit\n",
    "fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)\n",
    "fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)\n",
    "fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)\n",
    "\n",
    "# Find the region inside the lines\n",
    "XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))\n",
    "# The region inside the lines is defined by the lines' equations. Conditions are:\n",
    "# y> Ax + B for left and right lines\n",
    "# y< Ax + B for bottom line\n",
    "# A and B are found using np.polyfit() above (fit_left, fit_right, fit_bottom)\n",
    "region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \\\n",
    "                    (YY > (XX*fit_right[0] + fit_right[1])) & \\\n",
    "                    (YY < (XX*fit_bottom[0] + fit_bottom[1]))\n",
    "\n",
    "# Color pixels red which are inside the region of interest\n",
    "region_select[region_thresholds] = [255, 0, 0]\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(region_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining Color and Region Selections\n",
    "\n",
    "Now you've seen how to mask out a region of interest in an image. Next, let's combine the mask and color selection to pull only the lane lines out of the image. \n",
    "\n",
    "Check out the code below. Here we’re doing both the color and region selection steps, requiring that a pixel meet both the mask and color selection requirements to be retained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Vary your color selection and the shape of your region mask (vertices of a triangle left_bottom, right_bottom, and apex), such that you pick out the lane lines and nothing else. We just want our ego-lane, not the neighboring lanes left or right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_select = np.copy(image)\n",
    "line_image = np.copy(image)\n",
    "\n",
    "# Define color selection criteria\n",
    "red_threshold = 0\n",
    "green_threshold = 0\n",
    "blue_threshold = 0\n",
    "\n",
    "rgb_threshold = [red_threshold, green_threshold, blue_threshold]\n",
    "\n",
    "# Define the vertices of a triangular mask.\n",
    "# Keep in mind the origin (x=0, y=0) is in the upper left\n",
    "left_bottom = [0, 539]\n",
    "right_bottom = [900, 300]\n",
    "apex = [400, 0]\n",
    "\n",
    "# Perform a linear fit (y=Ax+B) to each of the three sides of the triangle\n",
    "# np.polyfit returns the coefficients [A, B] of the fit\n",
    "fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)\n",
    "fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)\n",
    "fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)\n",
    "\n",
    "# Mask pixels below the threshold\n",
    "color_thresholds = (image[:,:,0] < rgb_threshold[0]) | \\\n",
    "                    (image[:,:,1] < rgb_threshold[1]) | \\\n",
    "                    (image[:,:,2] < rgb_threshold[2])\n",
    "\n",
    "# Find the region inside the lines\n",
    "XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))\n",
    "region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \\\n",
    "                    (YY > (XX*fit_right[0] + fit_right[1])) & \\\n",
    "                    (YY < (XX*fit_bottom[0] + fit_bottom[1]))\n",
    "                    \n",
    "# Mask color and region selection\n",
    "color_select[color_thresholds | ~region_thresholds] = [0, 0, 0]\n",
    "# Color pixels red where both color and region selections met\n",
    "line_image[~color_thresholds & region_thresholds] = [255, 0, 0]\n",
    "\n",
    "# Display the image and show region and color selections\n",
    "plt.imshow(image)\n",
    "x = [left_bottom[0], right_bottom[0], apex[0], left_bottom[0]]\n",
    "y = [left_bottom[1], right_bottom[1], apex[1], left_bottom[1]]\n",
    "plt.plot(x, y, 'b--', lw=4)\n",
    "plt.imshow(color_select)\n",
    "plt.show()\n",
    "plt.imshow(line_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Canny Edge Detection\n",
    "\n",
    "So you found the lane lines... simple right? Now you’re ready to upload the algorithm to the car and drive autonomously right?? Well, not quite yet ;)\n",
    "As it happens, lane lines are not always the same color, and even lines of the same color under different lighting conditions (day, night, etc) may fail to be detected by our simple color selection.\n",
    "What we need is to take our algorithm to the next level to detect lines of any color using sophisticated computer vision methods. \n",
    "\n",
    "**TODO:** Apply Gaussian blur and Canny edge detection to the `image2` below. This image is more challenging due to the yellow line. Store your resulting edge image in a variable called `masked_edges`. Plot your result. Which parameters work best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image and print some stats\n",
    "image2 = mpimg.imread('images/exit-ramp.jpg')\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY) #grayscale conversion\n",
    "print('This image is: ', type(image2), \n",
    "         'with dimensions:', image2.shape)\n",
    "\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE GOES HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have detected nice lane lines - but also a lot of other edges! So we need something better, which leads us to the Hough transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing a Hough Transform on Edge Detected Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know how the Hough Transform works, but to accomplish the task of finding lane lines, we need to specify some parameters to say what kind of lines we want to detect (i.e., long lines, short lines, bendy lines, dashed lines, etc.). \n",
    "To do this, we'll be using an OpenCV function called HoughLinesP that takes several parameters. Let's code it up and find the lane lines in the image we detected edges in with the Canny function (for a look at coding up a Hough Transform from scratch, check this out: https://alyssaq.github.io/2014/understanding-hough-transform/)\n",
    "\n",
    "Let's look at the input parameters for the OpenCV function HoughLinesP that we will use to find lines in the image. You will call it like this:\n",
    "```\n",
    "lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),\n",
    "                                             min_line_length, max_line_gap)\n",
    "```\n",
    "In this case, we are operating on the image masked_edges (the output from Canny) and the output from HoughLinesP will be lines, which will simply be an array containing the endpoints (x1, y1, x2, y2) of all line segments detected by the transform operation. The other parameters define just what kind of line segments we're looking for. \n",
    "\n",
    "- First off, rho and theta are the distance and angular resolution of our grid in Hough space. Remember that, in Hough space, we have a grid laid out along the (Θ, ρ) axis. You need to specify rho in units of pixels and theta in units of radians. \n",
    "So, what are reasonable values? Well, rho takes a minimum value of 1, and a reasonable starting place for theta is 1 degree (pi/180 in radians). Scale these values up to be more flexible in your definition of what constitutes a line.\n",
    "- The threshold parameter specifies the minimum number of votes (intersections in a given grid cell) a candidate line needs to have to make it into the output. \n",
    "- The empty np.array([]) is just a placeholder, no need to change it. \n",
    "- min_line_length is the minimum length of a line (in pixels) that you will accept in the output, and max_line_gap is the maximum distance (again, in pixels) between segments that you will allow to be connected into a single line. \n",
    " \n",
    "You can then iterate through your output lines and draw them onto the image to see what you got!\n",
    "So, here's what its going to look like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hough transform parameters\n",
    "# Make a blank the same size as our image to draw on\n",
    "rho = 1\n",
    "theta = np.pi/180\n",
    "threshold = 1\n",
    "min_line_length = 10\n",
    "max_line_gap = 1\n",
    "line_image = np.copy(image2)*0 #creating a blank to draw lines on\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),\n",
    "                            min_line_length, max_line_gap)\n",
    "\n",
    "# Iterate over the output \"lines\" and draw lines on the blank\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "# Create a \"color\" binary image to combine with line image\n",
    "color_edges = np.dstack((masked_edges, masked_edges, masked_edges)) \n",
    "\n",
    "# Draw the lines on the edge image\n",
    "combo = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0) \n",
    "plt.imshow(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I've detected lots of line segments! Your job, in the next exercise, is to figure out which parameters do the best job of optimizing the detection of the lane lines. Then, you'll want to apply a region of interest mask to filter out detected line segments in other areas of the image. Earlier in this lesson you used a triangular region mask, but this time you'll get a chance to use a quadrilateral region mask using the cv2.fillPoly() function (keep in mind though, you could use this same method to mask an arbitrarily complex polygon region). When you're finished you'll be ready to apply the skills you've learned to do the assignment at the end of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting it all together\n",
    "\n",
    "**TODO:** Now it's your turn to play with the Hough Transform on an edge-detected image. Your job is to modify the parameters for the Hough Transform and impose a region of interest mask to get output that does a better job finding just the lane lines than the image above. In the code, I've given you a framework for defining a quadrilateral region of interest mask. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and grayscale the image\n",
    "image2 = mpimg.imread('images/exit-ramp.jpg')\n",
    "gray2 = cv2.cvtColor(image2,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Define a kernel size and apply Gaussian smoothing\n",
    "kernel_size = 5\n",
    "blur_gray = cv2.GaussianBlur(gray2,(kernel_size, kernel_size),0)\n",
    "\n",
    "# Define our parameters for Canny and apply\n",
    "low_threshold = 50\n",
    "high_threshold = 150\n",
    "edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "# Next we'll create a masked edges image using cv2.fillPoly()\n",
    "mask = np.zeros_like(edges)   \n",
    "ignore_mask_color = 255   \n",
    "\n",
    "# This time we are defining a four sided polygon to mask (we can still define a triangular region with 4 vertices, but we are more flexible)\n",
    "imshape = image2.shape\n",
    "vertices = np.array([[(0,imshape[0]),(0, 0), (imshape[1], 0), (imshape[1],imshape[0])]], dtype=np.int32)\n",
    "cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "masked_edges = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "# Plot the rectangular region mask\n",
    "plt.imshow(masked_edges, cmap='Greys_r')\n",
    "plt.plot(vertices[0][:, 0], vertices[0][:, 1], 'b--', lw=4)\n",
    "plt.show()\n",
    "\n",
    "# Define the Hough transform parameters\n",
    "# Make a blank the same size as our image to draw on\n",
    "rho = 1 # distance resolution in pixels of the Hough grid\n",
    "theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "threshold = 1     # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_length = 5 #minimum number of pixels making up a line\n",
    "max_line_gap = 1    # maximum gap in pixels between connectable line segments\n",
    "line_image = np.copy(image2)*0 # creating a blank to draw lines on\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "# Output \"lines\" is an array containing endpoints of detected line segments\n",
    "lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),\n",
    "                            min_line_length, max_line_gap)\n",
    "\n",
    "# Iterate over the output \"lines\" and draw lines on a blank image\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "# Create a \"color\" binary image to combine with line image\n",
    "color_edges = np.dstack((edges, edges, edges)) \n",
    "\n",
    "# Draw the lines on the edge image\n",
    "lines_edges = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0) \n",
    "plt.imshow(lines_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now know the basics of how autonomous cars can detect lane lines using Hough transform!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters\n",
    "Parameter tuning is one of the biggest challenges in computer vision - what works well for one image may not work at all with different lighting and/or backgrounds.\n",
    "Computer Vision Engineers gain an intuition over time for ranges of parameters and different techniques that might work best for a set of situations. When getting started, this can be a big hill to climb. Oftentimes, building a tool to help speed up your iteration between different techniques and thresholds can help you in parameter tuning.\n",
    "While it's not required for the project, you might consider reading this blog post from a Self-Driving Car student on their approach to a parameter tuning tool, and consider building one of your own in the future! https://medium.com/@maunesh/finding-the-right-parameters-for-your-computer-vision-algorithm-d55643b6f954"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
